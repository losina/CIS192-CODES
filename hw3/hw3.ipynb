{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS 192\n",
    "## Homework 3: Acquiring and Processing Data\n",
    "### Name: Jina Lo\n",
    "\n",
    "**Directions:** For this homework you will learn how to use Jupyter notebooks and some common Python packages for data mining, modeling, and visualization leading up to next week's introduction to data science. This notebook is split into two parts:\n",
    "1. ([An introduction to Jupyter](#An_introduction_to_Jupyter_notebooks)). For this section, just **follow along the instruction texts and run the code cells** (see below for instructions for running cells). Feel free to change the code cells.\n",
    "2. ([The homework portion](#HW_Portion)). For this section, complete the map and text acquiring and processing data assignment however you want by adding your own code cells. In the end, you will **export a data file, and you will submit that along with this notebook**.\n",
    "\n",
    "If you feel like you are already an expert with Jupyter notebooks, feel free to skip right to ([The homework portion](#HW_Portion)). \n",
    "\n",
    "**Note**: If you're running into any unexpected errors while working through this notebook, come to office hours or ask a question on [Piazza](https://piazza.com/class/k05elkoynkb6lx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An_introduction_to_Jupyter_notebooks\n",
    "\n",
    "A Jupyter notebook (formally iPython) is an interactive environment for Python, and it's probably the best way of using Python for data manipulation.  You may ask: \"I can just run python interactively from the terminal, why do I need jupyter?\"  Well, that's a fair question, and the answer will hopefully become clear as we work through this notebook.\n",
    "\n",
    "Jupyter notebooks are broken down into **cells**.  We're in the topmost cell of this notebook at the moment.  Cells come in three flavors:\n",
    "\n",
    "* **Markdown cells** allows you to edit the text in [Markdown](https://guides.github.com/features/mastering-markdown/).  These cells are used for exposition, discussion, and general formatting.  Think of them as extended comments that can be formatted beautifully, and can contain [links](http://www.jupyter.org), bulleted lists, etc.  Anything that Markdown can!  They can even contain $\\LaTeX$ code ([huh?](#footer_latex)): '$$\\left(\\int_1^{\\sqrt[3]{3}}z^2 dz \\right)\\cos\\left(\\frac{3\\pi}{9}\\right) = \\ln\\left(\\sqrt[3]{e}\\right)$$\n",
    "* **Code cells** contain code (for us, Python code).  These cells can contain code as short as one line, or as long as you'd like!  (Actually, I have no idea what the maximum length is.  I've had cells well over 100 lines long though).  They have some basic text editor support, so they'll help you with indentation, tab completion, etc., but they won't be able to do some of the magic that true editors like Atom or Sublime can handle.  They're also interactive in the same way that the Python interpreter in interactive mode is.  Type `15*4 %3` and it returns the answer, no need to print out everything.\n",
    "\n",
    "There's one more, but it's not used as often:\n",
    "\n",
    "* **Raw cells** are used when you want to hack the notebook to make it fancier.  We won't be using them tons, but it's good to know they exist.\n",
    "\n",
    "## Downloading Jupyter\n",
    "To get started, first download Jupyter following the instructions [here](https://jupyter.readthedocs.io/en/latest/install.html). Then, download this notebook, run `jupyter notebook` from your terminal where you downloaded the file, open the notebook, and continue to follow on with the rest of the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration Examples\n",
    "How about a little demonstration?  Try running the cell below by highlighting it and pressing either `cmd + enter` or `ctrl + enter`.  If that *doesn't* work, then you're not [running the notebook](https://jupyter.readthedocs.io/en/latest/running.html#running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:08:38.990647Z",
     "start_time": "2019-09-17T19:08:38.982802Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    \"\"\" Determine whether n is prime.\"\"\"\n",
    "    k = 2\n",
    "    while k*k <= n:\n",
    "        if n % k == 0:\n",
    "            return False\n",
    "        k += 1\n",
    "    return True\n",
    "\n",
    "\n",
    "print(*[x for x in range(2, 401) if is_prime(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Side note** for people confused by that last line](#footer_list_comprehensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:08:54.356039Z",
     "start_time": "2019-09-17T19:08:54.351953Z"
    }
   },
   "outputs": [],
   "source": [
    "# As proof that the function is in memory, let's query the function\n",
    "print(is_prime, type(is_prime), is_prime(57), sep=\"\\n\"*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing a notebook: Command mode and Edit mode\n",
    "\n",
    "While working with a notebook, you are always in one of two modes.\n",
    "\n",
    "1. In **Edit mode** you can edit the content of a cell.  It acts like a text file inside a text editor, and has some helpful syntax highlighting.  If you're editing a Markdown cell, it will look significantly different.  If you're editing a code cell, it will look mostly the same.  To *run* the cell, you have a few options:\n",
    " * press `command + enter` or `ctrl + enter` to run the cell and exit edit mode.  Running a markdown cell with render it, and running a code cell performs as you expect.\n",
    " * press `shift + enter` to run the cell and insert a new cell below.  This is the standard command then you're building the notebook.\n",
    "  * You can also press `esc` to go to command mode without running the cell.\n",
    "1. In **Command mode** you have access to your cells in a larger-scale way.  You can press `up` or `down` to move between cells, and press `enter` to enter edit mode on the currently selected cells.  You can also cut, copy, paste, and delete cells with appropriate keyboard commands.  Open the *Command Palette* (the keyboard in the top center of the toolbar) to see all the commands you can use in Command mode.\n",
    "\n",
    "## Linearity of code: the kernel\n",
    "\n",
    "A notebook has a **kernel** attached to it.  Think of it as the interactive python running behind the scenes, executing your commands when you send them.  There are two forms of *linearity*, or continuity-of-your-work, going on here, and it can be a bit confusing to new Jupyter users:\n",
    "\n",
    "* **Kernel Linearity**: After you execute a code cell, it gives you its output and places a number next to the top left corner of the cell.  This number is the *order of cell execution* in the kernel.  It's the order the kernel received from you.  This means you can run cells, tweak them and run them again, run something \"below\" the cell in the notebook, then come back and run the upper cell, *etc.*, and the kernel will keep track of this in terms of the order in which you ran them **chronologically**.  This is the order you want to keep in mind.  It's really useful!  You can start out with a junky-looking notebook, figure out your data analysis, realize you want to change stuff \"in the past\", and just go back and change them.  Once you get used to this, you'll love it.\n",
    "* **Cell Linearity**: There is an obvious order to the cells: the top ones \"go first\", and the lower ones \"go next\".  This isn't exactly necessary, though.  It definitely is the goal of the *final product* to go linearly, but programming, and especially data analysis, isn't like writing a journal entry.  Very often, you'll need to go back and change things, then rerun all the cells that come after the one you just edited.  You may type one line in a cell, hit `shift + enter` to see the output and move on to the next cell, then do that three more times.  You then realize that you'd prefer to have done all that at once, and you can merge those three cells together.  It's a workflow that I hope you'll learn to love.\n",
    "\n",
    "Play around with it now: we first use an uninitialized variable `my_hat` in a cell; hit `shift + enter` to see the error.  Below that, we create a cell in which we give the variable a value, then run that cell, followed by the original cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:08:59.104205Z",
     "start_time": "2019-09-17T19:08:58.996030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Push shift+enter to see an error or esc to not run the cell:\n",
    "\n",
    "print(my_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:09:00.433700Z",
     "start_time": "2019-09-17T19:09:00.430860Z"
    }
   },
   "outputs": [],
   "source": [
    "my_hat = \"Oh, now it works!\"\n",
    "\n",
    "# run this cell, then run the above cell!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll get the hang of it in time.  Another way this can bite you is by rerunning the same cell and expecting a certain result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:09:03.375399Z",
     "start_time": "2019-09-17T19:09:03.372609Z"
    }
   },
   "outputs": [],
   "source": [
    "# run this cell once!\n",
    "title = \"The Cat in the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:09:07.036421Z",
     "start_time": "2019-09-17T19:09:07.033110Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# run this cell many times!\n",
    "\n",
    "title += \" Hat\"\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing to note about jupyter notebooks is that, unlike the Python interpreter, you have full access to your shell (bash or cmd, most likely) by using the `!` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:10:06.270655Z",
     "start_time": "2019-09-17T19:10:06.121138Z"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"Hello from a text file!\" > hello.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:10:21.887842Z",
     "start_time": "2019-09-17T19:10:21.763991Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the file we just made:\n",
    "\n",
    "# I'm on a windows machine as I make this notebook, so I'll use:\n",
    "# !dir\n",
    "\n",
    "# But if you're on a *nix machine (such as a macbook) you should use:\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:10:32.473702Z",
     "start_time": "2019-09-17T19:10:32.466529Z"
    }
   },
   "outputs": [],
   "source": [
    "# If there's no `!`, then we're using Python.  Here's the\n",
    "# Python command to open the text file we just made:\n",
    "with open(\"hello.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:10:43.126825Z",
     "start_time": "2019-09-17T19:10:43.003175Z"
    }
   },
   "outputs": [],
   "source": [
    "# Again, the windows commands:\n",
    "# !del hello.txt\n",
    "\n",
    "# But if you're on a *nix machine you should use:\n",
    "!rm hello.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can now go run the above `!dir` or `!ls` cell to double check that the file is gone!  Remember, *kernel linearity* is the important thing to keep in mind.)  I mostly use `!` in order to `!pip install <whatever_python_library_I_need>`.  (What's [pip](https://pythonprogramming.net/using-pip-install-for-python-modules/)?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:10:47.200612Z",
     "start_time": "2019-09-17T19:10:45.300952Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this makes a somewhat long printout.  If your final submission of a notebook, you should remove all the long printouts to have a more readable notebook.  Now let's move on to something more interesting.\n",
    "\n",
    "## Visualizing Datasets\n",
    "\n",
    "Jupyter notebooks are a great way to work with data. In this section, we will\n",
    "1. Learn how to use the csv package, and learn how to use the staple Python library Pandas, which provides a DataFrame object, which is essentially a numpy array with lots of extra methods attached to it.\n",
    "2. Work with a package called matplotlib to visualize some data. \n",
    "To get started, download the necessary packages and [matplotlib](https://matplotlib.org/downloads.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:24.012409Z",
     "start_time": "2019-09-17T19:11:23.669136Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:26.122995Z",
     "start_time": "2019-09-17T19:11:26.114116Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, 10, .1)\n",
    "y = x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:27.655995Z",
     "start_time": "2019-09-17T19:11:27.459954Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic data analysis with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:33.234712Z",
     "start_time": "2019-09-17T19:11:31.380966Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard import statements for data analysis\n",
    "import numpy as np                  # linear algebra library\n",
    "import pandas as pd                 # data analysis and manipulation\n",
    "import matplotlib.pyplot as plt     # standard-issue plotting library\n",
    "import seaborn as sns               # fancier plotting library\n",
    "\n",
    "# other helpful libraries\n",
    "import requests                     # allows the use of HTTP requests\n",
    "# turns a string into an input stream so that pandas can load it.\n",
    "from io import StringIO\n",
    "# allows for operating-system-specific path joining\n",
    "from os.path import join\n",
    "\n",
    "# The following line is a Jupyter \"magic\": lines beginning with a `%` are how you talk to Jupyter\n",
    "#   (instead of Python or the shell) .  Here, I'm telling Jupyter to display matplotlib plots\n",
    "#   as inline, as opposed to the default of having them pop up in their own window, buried\n",
    "#   behind everything else.\n",
    "%matplotlib inline\n",
    "\n",
    "# Use the Requests library to pull a dataset from the internet\n",
    "response = requests.get(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\n",
    "\n",
    "# Get just the data\n",
    "data_string = response.text\n",
    "\n",
    "print(response.text[:279])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a mess!  Let's turn that into something a bit more manageable.  Enter pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:36.027745Z",
     "start_time": "2019-09-17T19:11:35.991198Z"
    }
   },
   "outputs": [],
   "source": [
    "# `df` is short for dataframe, the standard object in Pandas (which we abbreviated as `pd`), the Python\n",
    "#    dataset manipulation library.  Think of it as Excel, but awesome.\n",
    "df = pd.read_csv(StringIO(data_string))\n",
    "\n",
    "# show me the top 5 rows!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it's prettier, but the columns are all messed up.  What gives?  Pandas figures that your first row is the names of the columns.  We'll add an extra line to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:39.207701Z",
     "start_time": "2019-09-17T19:11:39.193141Z"
    }
   },
   "outputs": [],
   "source": [
    "column_names = [\"sepal length\", \"sepal width\",\n",
    "                \"petal length\", \"petal width\", \"species\"]\n",
    "\n",
    "# read it in again to not lose the first line of data:\n",
    "df = pd.read_csv(StringIO(data_string), names=column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now this is a working dataset.  Let's do some basic checks on our data, just to see what we're working with.\n",
    "\n",
    "(By the way, a quick *meta note*: notice that I'm using these markdown cells to walk you through the process of my data analysis using this notebook.  It's **much** nicer to look through someone else's work and see what they've done when they tell you a bit of a story with their Markdown cells along the way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic pandas commands: data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:41.929168Z",
     "start_time": "2019-09-17T19:11:41.925246Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in our little `df.head()` call above, all the entries for `species` were the same.  That suggests there aren't too many *unique* options for `species` (In this case, `species` is *categorical data*: it doesn't really have any ordering).  How many unique options are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:44.402822Z",
     "start_time": "2019-09-17T19:11:44.391033Z"
    }
   },
   "outputs": [],
   "source": [
    "df['species'].unique()\n",
    "\n",
    "# note that I could have also said:\n",
    "# df.species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is the famous [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset.  It contains data on  three species of iris, with four interesting columns of data about them.  You can use this dataset to learn about plotting, making basic machine learning models, etc.  Let's continue to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:47.873627Z",
     "start_time": "2019-09-17T19:11:47.841020Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some useful statistics on the dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells you quite a bit about the dataset, if you understand what you're looking at.  `count` is the number of elements, `mean` is the mean of the data (note that this requires doing some math on the data, so it will often be messed up if you have an error in your dataset, like missing data), `std` is the standard deviation, and the rest are [percentiles](https://en.wikipedia.org/wiki/Percentile). From this, I can see that nothing looks our of place, like there are unlikely to be incorrect values ($-150$cm long petals, for example), so we're okay to move on to some *exploratory* plots of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic plotting\n",
    "As I mentioned above, Matplotlib is the standard plotting library for Python. Let's make a simple scatter plot of two of the columns, to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:52.276839Z",
     "start_time": "2019-09-17T19:11:52.272605Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:11:56.544254Z",
     "start_time": "2019-09-17T19:11:56.315335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: the semi-colon is optional and makes the output slightly prettier.\n",
    "plt.scatter(df['petal length'], df['petal width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool, that was pretty easy.  However, it's not currently telling me much about that important-seeming lower left chunk of data points.  Is it maybe just one species, and the upper bit is the other two?  We can add some color to our plot to test this hypothesis.  To do that, we need to feed a list of colors that's the same length as the data (a length-150 list of strings like `\"red\", \"blue\"`, *etc.*) based on the species.  We'll make a Python dictionary to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:00.496447Z",
     "start_time": "2019-09-17T19:12:00.486644Z"
    }
   },
   "outputs": [],
   "source": [
    "color_options = [\"red\", \"blue\", \"green\"]\n",
    "\n",
    "# overly fancy for this example, but very useful when you have tons of categories in your variable!\n",
    "color_mapping = {species: color for species,\n",
    "                 color in zip(df['species'].unique(), color_options)}\n",
    "\n",
    "colors = [color_mapping[species] for species in df['species']]\n",
    "\n",
    "print(*colors, sep=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty long and ugly printout.  I usually wouldn't leave that in the notebook, but I'm showing you what we're creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:03.195007Z",
     "start_time": "2019-09-17T19:12:02.978802Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['petal length'], df['petal width'], c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis was correct!  Awesome!  But, which species does this single out?  \"The red one\" is a pretty unacceptable answer; we need a legend!  Here's a first guess: the matplotlib documentation says that `plt.legend` needs \"labels\", whatever that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:05.107983Z",
     "start_time": "2019-09-17T19:12:04.886718Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['petal length'], df['petal width'], c=colors)\n",
    "plt.legend(labels=df['species'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's... not right, and a bit silly.  It turns out that in order to do this with basic matplotlib, we actually have to split our data up based on its species (category).  (You're probably thinking \"Wait, really? That's a silly way to do it!\", and you wouldn't be wrong.  Stay with me!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:07.322766Z",
     "start_time": "2019-09-17T19:12:07.064435Z"
    }
   },
   "outputs": [],
   "source": [
    "for species in df['species'].unique():\n",
    "    # Restrict the dataset to just those rows that are this species\n",
    "    df_restricted = df.loc[df['species'] == species]\n",
    "\n",
    "    # Note that you can also just put in a single color to the `c` option:\n",
    "    plt.scatter(df_restricted['petal length'],\n",
    "                df_restricted['petal width'],\n",
    "                label=species,                  # This allows the legend to work\n",
    "                c=color_mapping[species])\n",
    "\n",
    "# Since we labeled the individual calls to `plt.scatter`, we can just use:\n",
    "plt.legend()\n",
    "\n",
    "# Always label your axes!\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, *that's* a nice graph.  But it sure was a decent amount of work to get it!  The library Seaborn (which is just a wrapper around Matplotlib with a bunch of convenience functions) has some helpful tools to get us this very straightforward graph more quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:10.405661Z",
     "start_time": "2019-09-17T19:12:10.086863Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot('petal length',\n",
    "           'petal width',\n",
    "           data=df,\n",
    "           hue='species',       # color the data by the 'species' column\n",
    "           fit_reg=False,       # don't bother fitting a linear regression line\n",
    "           legend_out=False)   # place the legend inside the graph, not outside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last **awesomely useful** plot that Seaborn can make for you, especially for machine learning applications, is called a [pair plot](https://www.quora.com/What-are-pair-plots).  It compares all the different columns/variables against each other using scatter plots, and then plots histograms along the \"diagonal\", where the \"row\" and \"column\" variables of the plot are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:18.680567Z",
     "start_time": "2019-09-17T19:12:15.516880Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with messy data\n",
    "\n",
    "Pandas is also really good at helping you if you have messy data.  The iris dataset that we've been working with is *famously* a clean, perfect dataset.  However, most are not, and certainly not the ones that you construct yourself. The problem of what to do with [missing data](https://en.wikipedia.org/wiki/Missing_data) is a *huge* one, and entire branches of statistics have been set up to deal with it. The topic of what to do with missing data is a big one, and not something we can cover in a single cell.  The simplest solution is to [*impute*](https://en.wikipedia.org/wiki/Imputation_%28statistics%29) them to be some kind of average or otherwise representative value. \n",
    "\n",
    "Another common example of messy data is \"NaN\" columns stand for [not a number](https://en.wikipedia.org/wiki/NaN); they indicate missing data. The term [tidy data](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) means something very specific, and is a good, industry standard goal to work toward in your data cleaning process.\n",
    "\n",
    "For the HW portion of this notebook, you will learn about a few more common data cleaning steps.\n",
    "\n",
    "<!-- Small side note: notice I used \"1.\" for both items and it updated automatically.  That's the beauty of markdown!  \n",
    "\n",
    "By the way, if you're reading this, you've entered edit mode for this cell.  I wanted to call attention to the thing above, but only if you can see it, so I wanted to add THIS text only if you're in edit mode.  How is this possible?  Well, markdown is secretly just pre-processed HTML, and this text is inside of an HTML comment, so that's why you can't see it in the final version of the cell.  The more you know! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW_Portion\n",
    "\n",
    "**Directions:** Throughout this homework portion, add markdown cells as needed in order to document your process of trying to understand it.  I don't expect to just see three perfect answers and no explanations.  Tell me what you are trying!  In the end, make sure your cell and kernel linearities line up (that is, make sure that your final product can be followed from top to bottom).\n",
    "\n",
    "Our goal is to gain a bit more familiarity with the way you might generate datasets from [APIs](https://en.wikipedia.org/wiki/Application_programming_interface) or from a larger CSV.\n",
    "\n",
    "**Make sure you submit a zip file with:**\n",
    "1. this **notebook** (provided)\n",
    "2. the **locations.json** output from part 1\n",
    "3. **bnc-wordfreq.csv** input from part 2\n",
    "4. **raven.txt** (provided)\n",
    "5. the **raven.csv** output from part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Google Maps Geocoding API\n",
    "\n",
    "Below is a function that generates a random Latitude and Longitude in Wyoming (it's a [particularly square state](https://www.mapsofworld.com/usa/states/wyoming/wyoming-maps/wyoming-lat-long-map.jpg)).  **Your first goal** is to use the Google Maps Geocoding API to create a dataset of 10 random locations in Wyoming and the **town and zip code** they lie in.  For example:\n",
    "```\n",
    "[{\"lat\": \"44.952055\", \"lon\": \"-107.67753\", \"town\": \"Parkman\", \"zip\": \"82838\"}, ...]\n",
    "```\n",
    "\n",
    "To do this: \n",
    "\n",
    "1. [Get a google maps geocoding API key](https://developers.google.com/maps/documentation/geocoding/get-api-key) -- It's free and quick, just tell them the name of your \"app\" (it can be anything)\n",
    "2. [Take a look at how to use the geocoding API](https://developers.google.com/maps/documentation/geocoding/start) -- You're looking for the process they call \"reverse geocoding\"\n",
    "3. Build your request\n",
    "4. Interpret the result: it's a JSON response with tons of extra data so let me help you: you'll need `your_request_response_object.json()`.  Just grab the first result's `address_components` and dive into that array, or look for the results `formatted_address` and find the town and zip code in the resulting string (try `my_string.split(,)`.)\n",
    "5. Save the data as a [JSON file](https://docs.python.org/2/library/json.html) (or [use pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html), possibly easier). Name it `*locations.json*`.\n",
    "\n",
    "Some tips:\n",
    "* When you're done cleaning a dataset, it's usually good to save the data to a **new** file.  It's very important that it's new, to help with [data lineage/provenance](https://en.wikipedia.org/wiki/Data_lineage).\n",
    "* If you don't want to add a billing account to you Google API, feel free to use a free gelocation API, for example, (https://ipstack.com/).\n",
    "* I recommend using a while loop with 10 iterations with a [try-except](https://docs.python.org/3/tutorial/errors.html), in case the API returns a bad response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:12:23.072881Z",
     "start_time": "2019-09-17T19:12:23.068696Z"
    }
   },
   "outputs": [],
   "source": [
    "# some necessary imports\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def generate_lat_long():\n",
    "    \"\"\" Generate a random latitude and longitude in Wyoming. \n",
    "    Bounds: 41°N to 45°N and 104.05°W to 111.05°W\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"{},{}\".format(round(random.uniform(41, 45), 5), round(random.uniform(-111.05, -104.05), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect 10 data entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "lat_long = []\n",
    "i = 0\n",
    "while i < 10: \n",
    "    try:\n",
    "        ran = generate_lat_long().split(\",\")\n",
    "        response = requests.get(\"https://maps.googleapis.com/maps/api/geocode/json?latlng=\" + ran[0] + \",\" + ran[1] + \"&key=AIzaSyAfbifULsxdnnQ3frwp-1WlZpCPRc2WqSU\")\n",
    "        response = response.json()[\"results\"][0]['address_components']\n",
    "        result = {\"lat\": ran[0], \"lon\": ran[1]}\n",
    "        for elem in response:\n",
    "            if 'locality' in elem['types']:\n",
    "                result['town'] = elem['long_name']\n",
    "            elif 'postal_code' in elem['types']:\n",
    "                result['zip'] = elem['long_name']\n",
    "        if \"zip\" in result and \"town\" in result:\n",
    "            lat_long.append(result)\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"some error in API\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now store the data into a json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(lat_long)\n",
    "df.to_json('locations.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Slicing a dataset using pandas\n",
    "\n",
    "[This CSV](http://introcs.cs.princeton.edu/java/data/bnc-wordfreq.csv) contains word frequencies in a subset of the British National Corpus, a 100 million long collect\n",
    "\n",
    "Questions/tasks for you:\n",
    "1. How many words are in this dataset?\n",
    "2. Construct the dataset consisting of all nouns whose frequency is greater than 20000 and which contain an \"`ag`\" in them.  Some hints: use pandas [boolean slicing](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing); pandas can help you with [string containment](https://pandas.pydata.org/pandas-docs/stable/text.html#testing-for-strings-that-match-or-contain-a-pattern), too.\n",
    "3. Construct the dataset of all the prime-number-indexed rows.  Use [`.loc`](https://pandas.pydata.org/pandas-docs/stable/indexing.html#basics)\n",
    "\n",
    "Both of the datasets may be just constructed in memory, _i.e._ no need to save them to a file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: How many words are in this dataset? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "response = requests.get(\n",
    "    \"https://introcs.cs.princeton.edu/java/data/bnc-wordfreq.csv\")\n",
    "\n",
    "data = response.text\n",
    "bnc_df = pd.read_csv(StringIO(data))\n",
    "bnc_df.to_csv('bnc-wordfreq.csv')\n",
    "words_num = len(bnc_df[\"WORD\"])\n",
    "print(words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are total 6318 words in this data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Constuct the dataset consisting of all nouns whose frequency is greater than 20000 and which contain an \"ag\" in them\n",
    "\n",
    "select the rows that satisfies the boolean condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_df = bnc_df[(bnc_df['PART OF SPEECH'] == \"n\") & (bnc_df['FREQUENCY'] > 20000) & (bnc_df[\"WORD\"].str.contains(\"ag\"))] \n",
    "noun_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Construct the dataset of all the prime-number-indexed rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function below checks whether the given input number is a prime number or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(num):\n",
    "    if num > 1:\n",
    "        if num == 2:\n",
    "            return True\n",
    "        for i in range(2,num):\n",
    "            if (num % i) == 0:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    " \n",
    "    else:\n",
    "         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prime numbers up to the desired range \n",
    "prime_ind = [x for x in range(bnc_df.shape[0]) if is_prime(x)]\n",
    "prime_df = bnc_df.loc[prime_ind]\n",
    "prime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text Analysis\n",
    "\n",
    "The task of taking text data and making it usable is tricky and can sometimes be time consuming.  Today, we'll keep it pretty simple.\n",
    "\n",
    "1. First, open the text file `raven.txt`, and copy its contents into a single string.\n",
    "2. Then, remove any character that isn't a letter, `\\n`, or (perhaps!) punctuation.  (hint: `\"a\" in \"cat\"` is `True` in Python, whereas `\"&\" in \"cat\"` is `False`.)\n",
    "3. `split` the text by \"sentence\" (more likely by line for this particular text file).  The \"sentences\" will become the rows of your dataset, and the occurrence of certain words will be your columns.  It might help to further `split` your sentences by word.\n",
    "4. Create a dataset from this based upon whether the words `of`, `nothing`, `raven`, and/or `chamber` appear in each sentence: each entry in your dataset will be `0` (this word/column **not** in this sentence/row) or `1` (this word/column appears in this sentence/row).\n",
    "5. Output your dataset to a CSV, named `*raven.csv*`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remove special charcters and divide the text into a sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raven.txt', 'r') as file:\n",
    "    data = file.read()\n",
    "data = \"\".join([c for c in data if c == \" \" or c == \"\\n\" or 65 <= ord(c) <= 90 or 97 <= ord(c) <= 122]).split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, iterate through each sentence to count for \"of\", 'nothing', 'raven' and 'chamber'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = []\n",
    "for sentence in data:\n",
    "    of, nothing, raven, chamber = 0, 0, 0, 0\n",
    "    if sentence != \"\":\n",
    "        for word in sentence.split(\" \"):\n",
    "            word = word.lower()      \n",
    "            if word == \"of\": of += 1\n",
    "            if word == \"nothing\": nothing += 1\n",
    "            if word == \"raven\": raven += 1\n",
    "            if word == \"chamber\": chamber += 1\n",
    "        data_set.append([of, nothing, raven, chamber])\n",
    "raven_df = pd.DataFrame(data_set, columns = ['of', 'nothing', 'raven', 'chamber'])\n",
    "raven_df.to_csv('raven.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### footer_latex\n",
    "$\\LaTeX$ is the *de facto* standard tool for typesetting mathematics and all things based upon in (*e.g.* science).  You separate your mathematical code using dollar signs, as you saw me do up there.  By the way, the math in that example above was both correct and adorable (Can you tell me why?).\n",
    "\n",
    "### footer_list_comprehensions\n",
    "What does that `print` statement above mean? Well, let's start with something easier:\n",
    "```\n",
    ">>> [x for x in range(2, 401)]\n",
    "```\n",
    "This is equal to [2, 3, 4, ..., 400] (a list!).  This is called a [list comprehension](https://docs.python.org/2/tutorial/datastructures.html#list-comprehensions).\n",
    "```\n",
    ">>> [x for x in range(2, 401) if is_prime(x)]\n",
    "```\n",
    "This is all the primes between 2 and 400\n",
    "```\n",
    ">>> print(*[<that_command_above>])\n",
    "```\n",
    "The asterisk is sometimes called the *splat operator*, it \"unpacks\" the list.  Thus, `my_func(*[2,3,4])` is equivalent to `my_func(2,3,4)`."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "300px",
    "width": "368px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "40px",
    "left": "1083px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
